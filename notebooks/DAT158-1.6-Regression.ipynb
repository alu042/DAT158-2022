{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.S. Lundervold, v. 290822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Understand-the-problem-and-look-at-the-big-picture\" data-toc-modified-id=\"Understand-the-problem-and-look-at-the-big-picture-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Understand the problem and look at the big picture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Frame-the-problem\" data-toc-modified-id=\"Frame-the-problem-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Frame the problem</a></span></li><li><span><a href=\"#Select-performance-measures\" data-toc-modified-id=\"Select-performance-measures-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Select performance measures</a></span></li></ul></li><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Explore-the-data\" data-toc-modified-id=\"Explore-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Explore the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-distributions\" data-toc-modified-id=\"Feature-distributions-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Feature distributions</a></span></li><li><span><a href=\"#Converting-the-features'-data-types\" data-toc-modified-id=\"Converting-the-features'-data-types-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Converting the features' data types</a></span></li><li><span><a href=\"#Feature-encoding\" data-toc-modified-id=\"Feature-encoding-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Feature encoding</a></span></li><li><span><a href=\"#Setting-up-our-$f:-X-\\to-y$\" data-toc-modified-id=\"Setting-up-our-$f:-X-\\to-y$-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Setting up our $f: X \\to y$</a></span></li></ul></li><li><span><a href=\"#Create-training-and-test-sets\" data-toc-modified-id=\"Create-training-and-test-sets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create training and test sets</a></span></li><li><span><a href=\"#Data-preprocessing:-Data-cleaning,-feature-scaling-and-imputing-missing-data\" data-toc-modified-id=\"Data-preprocessing:-Data-cleaning,-feature-scaling-and-imputing-missing-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data preprocessing: Data cleaning, feature scaling and imputing missing data</a></span></li><li><span><a href=\"#Training-a-regression-model\" data-toc-modified-id=\"Training-a-regression-model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Training a regression model</a></span></li><li><span><a href=\"#Evaluating-models-/-performance-measures\" data-toc-modified-id=\"Evaluating-models-/-performance-measures-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Evaluating models / performance measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mean-absolute-error\" data-toc-modified-id=\"Mean-absolute-error-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Mean absolute error</a></span></li><li><span><a href=\"#Mean-squared-error-and-root-mean-squared-error\" data-toc-modified-id=\"Mean-squared-error-and-root-mean-squared-error-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Mean squared error and root mean squared error</a></span></li><li><span><a href=\"#In-scikit-learn\" data-toc-modified-id=\"In-scikit-learn-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>In scikit learn</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reviews some core concepts related to **regression** in machine learning based on concrete examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use two data sets for this, of increasing complexity: _vehicles_ and _housing prices_. This notebook goes through the _vehicles_ example. You'll study the housing data in Assignment #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/cars.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also look at strategies and techniques for evaluating models beyond those explored in previous notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import our standard framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory in which to store data\n",
    "NB_DIR = Path.cwd()     \n",
    "DATA = NB_DIR/'data'/'vehicles'     \n",
    "\n",
    "DATA.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the problem and look at the big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task will be to predict the price of a car from various descriptive features. One can imagine using this to determine whether an offered price is fair or, if you're a car dealer, to decide your sale price. Such a predictive model can also be used to see if there are interesting general trends linking the cost of the car to its features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we imagine that our model will be used as part of a more comprehensive pricing system, the broader picture may influence the performance measures we'd like to use.\n",
    "\n",
    "In this case, we keep things simple: we just want the predicted price to, on average, correspond to the actual sale price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two widely used performance measures for regression problems are the ***Root Mean Squared Error*** (RMSE) and the ***Mean Absolute Error*** (MAE). \n",
    "\n",
    "We'll talk more about these later in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the data provided by Nehal Birla here: https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho. Store it in the `DATA` directory to continue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !wget https://github.com/alu042/DAT158-2022/raw/main/notebooks/data/vehicles/archive.zip\n",
    "    shutil.unpack_archive('archive.zip', extract_dir=DATA)\n",
    "else:\n",
    "    shutil.unpack_archive(DATA/'archive.zip', extract_dir=DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different data sets. Let's have a quick look at them to decide which one to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(DATA.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = pd.read_csv('data/vehicles/car data.csv')\n",
    "car_details = pd.read_csv('data/vehicles/CAR DETAILS FROM CAR DEKHO.csv')\n",
    "car_details_v3 = pd.read_csv('data/vehicles/Car details v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_details.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_details_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_details_v3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the last one as it has the most features and instances. Note that there are some missing values in `mileage`, `engine`, `max_power`, `torque` and `seats` that we'll have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = car_details_v3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of the price distribution in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.histplot(df.selling_price, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some very expensive cars in the data set, but only a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the distribution of model years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.histplot(df.year, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the cars are quite new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a relationship between the model year and the price? Let's make a new categorical feature to investigate. Based on the above histogram, we say that cars from before 2010 are \"old\", between 2010 and 2015 are \"medium\" and 2015-2020 are \"new\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"age_cat\"] = pd.cut(df[\"year\"], bins=[1982, 2010, 2015, 2020],\n",
    "                               labels=['old', 'medium', 'new'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.histplot(data=df, x='selling_price', hue='age_cat')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a tendency for newer cars to be more expensive than older ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about transmission type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.histplot(df.transmission)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are manual transmission. Is there a relationship between the price and the type of transmission?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.histplot(data=df, x='selling_price', hue='transmission')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the automatic transmission cars are pricier. We can see this also by computing their mean prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find all the rows corresponding to automatic transmission, \n",
    "# extract their selling prices, and compute the mean\n",
    "df.loc[df.transmission=='Automatic'].selling_price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.transmission=='Manual'].selling_price.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the fuel and the price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fuel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.histplot(data=df, x='selling_price', hue='fuel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: further exploration using `pandas-profiling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data exploration is such a fundamental component of machine learning, many tools have been created to support the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas-profiler` is a convenient library to quickly get some insights into a data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !pip install --upgrade visions\n",
    "    !pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ProfileReport(df, minimal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the features' data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other features we could investigate in a similar way. But we're quickly faced with the problem that some of them, like the mileage, are numbers, but not stored as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should convert some of the features stored as strings (`object`) to integers and floats. Specifically, the mileage, the engine size and the max power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mileage.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we remove the units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max_power.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mileage'] = df.mileage.str.replace(' kmpl', '')\n",
    "df['mileage'] = df.mileage.str.replace(' km/kg', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert to floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mileage'] = df['mileage'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do similarly for the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.engine = df.engine.str.replace(' CC', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max_power = df.max_power.str.replace(' bhp', '')\n",
    "df.max_power = df.max_power.replace('', np.nan)        # Empty strings replaced by NaNs\n",
    "df.max_power = df.max_power.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than dealing with the heterogeneity of the torque feature, we'll simply drop it (feel free to do otherwise on your own!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('torque', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also drop the name of the car. This is to simplify things. A better idea would be to use it to extract information about the make and model of the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our machine learning models to work, we must represent the categorical features `fuel`, `transmission`, `seller_type`, and `owner` as numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to best do such feature encoding is a relatively large topic. The short version is that we can either do a *one hot encoding* if the feature values are not related to each other in some *ordinal* way (i.e., there's no reason to treat one as \"larger\" than the other), otherwise use an ordinal encoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, `fuel` and `transmission` are not ordinal features, while `owner` is (as it is the number of owners). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas to do the one hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['fuel'])\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['transmission'])\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['seller_type'])\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've stored the fuel and transmission information in one hot encoded vectors we can drop the original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['fuel', 'transmission', 'seller_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `owner` we'd like to keep the ordinal relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.owner.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('Test Drive Car', 0, inplace=True)\n",
    "df.replace('First Owner', 1, inplace=True)\n",
    "df.replace('Second Owner', 2, inplace=True)\n",
    "df.replace('Third Owner', 3, inplace=True)\n",
    "df.replace('Fourth & Above Owner', 4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our $f: X \\to y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store the features in 'X' and the labels in 'y'. Our goal is to approximate the function mapping `X` to `y`, where `y` is the `selling_price`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/f_xy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('selling_price', axis=1)\n",
    "y = df['selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To stress a point repeated multiple times already: We're not interested in how well our models perform on the training set; what we're after is how well they generalize to unseen data. \n",
    "\n",
    "The test set is meant to simulate unseen data (and should, therefore, not be touched when constructing and tuning our models). \n",
    "\n",
    "<img width=50% src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/testsplit.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is vital to ensure that the test set is a representative sample of the data. In our case, we want to ensure that it contains cars of all prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should base our decision on how to split the data on the explorations we've done above and on what the model is supposed to be used for (as that influences the kind of generalization estimate we want). For example, perhaps it is important to use the car's age as part of the decision.  Or the number of seats it has (maybe we find it important that the test set contains at least some two-seaters). And so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we'll simply ensure that the test set contains at least some expensive cars by performing a _stratified split_ on our new categorical feature representing the cars' expensiveness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=X.age_cat, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 6096 instances for training, 2032 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their car age distributions are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train.age_cat, alpha=0.5, label='train')\n",
    "plt.hist(X_test.age_cat, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the split we can drop `age_cat` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('age_cat', axis=1)\n",
    "X_test = X_test.drop('age_cat', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing: Data cleaning, feature scaling and imputing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the data to train machine learning models, we need to make sure it is \"clean\", the features are scaled, and consider how to deal with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from earlier that we can scale the features using, for example, scikit-learn's `StandardScaler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general strategy for dealing with missing data is to **impute**. In other words, insert data where there's none. This can be done in many ways, and a part of a comprehensive model selection design would. in practice. be dedicated to figuring out good imputing strategies. Sometimes, simply putting in the mean or median value calculated from all the instances having values for a given feature is an OK strategy. Other times one should try to be a bit more clever and use characteristics of the instance to decide what to put in for a missing value. For example, try to find the most similar instances in terms of the other features, then put in the mean or median value computed only from those. Or perhaps train machine learning models to perform the imputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we go for a simple strategy of imputing using the mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Explore other scaling and imputation strategies available in scikit-learn. For imputation, try these as starting points: <br><br>\n",
    "https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/<br> https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for classification, we have a lot of choices when building our model. For now, we'll use one of the standard built-in models in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained on the training data, and we can use it to make predictions for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the 2032 predictions from the Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred), y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the correct answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test), np.array(y_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put them next to each other and print out the first few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(y_test, y_pred))[:10] # \"Zip\" the two above arrays and display the first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model is close to correct some times, and way off for others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a scatter plot to compare the predicted prices agains the actual prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(x=y_test, y=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that at least the model isn't terrible.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **But how good is it, really? Can we quantify its performance?** \n",
    "\n",
    "As we did for classification earlier, we need metrics that we can use to evaluate our models. Again, as before, we can use these to compare different models and choice of model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating models / performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, as mentioned earlier, one should really ask, \"*What is the end goal for my system\"?* We're supposed to create systems that are useful in some context as part of a larger system, which typically has a higher-level goal that our system should aim to optimize. Perhaps it's worth sacrificing predictive performance for speed or not getting a lot of prices that don't lead to sales?\n",
    "\n",
    "However, we won't consider these broader context matters in these toy problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic idea for performance measures in regression is to compute the distances between the predicted and actual values for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(x=y_test, y=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the distances from the points in the above figure and the straight line $y=x$. In other words, the **errors** in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These distances are computed by taking the absolute value of the difference between the prediction and the target values, $|\\hat{y_i} - y_i|$, where $\\hat{y}_i$ is the predicted value for instance number i and $y_i$ is the true value. Technically, this is the [**euclidean distance**](https://en.wikipedia.org/wiki/Euclidean_distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the predictions are perfect, all these distances will be zero. The larger the distance, the more severe the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sum up all the distances and divide the result by the number of instances, we'll get the _average_ or _mean_ error. To avoid having the distances of the points above the line cancel out the distances of points below the line, we take the absolute value of each distance (to get a positive number). This gives us the following sum (where $n$ is the total number of instances):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(|\\hat{y_1} - y_1| + |\\hat{y_2} - y_2| + |\\hat{y_3} - y_3| + ... + |\\hat{y_n} - y_n|)\\, / \\, n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac 1n \\sum_i^n |\\hat{y_i} - y_i| = MAE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the so-called **mean absolute error**. \n",
    "\n",
    "Note again that if the predictions are perfect, this sum is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate the above formula into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between the two \n",
    "    vectors v1 and v2 (that are of equal length)\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0\n",
    "    # Add the absolute difference of all the elements \n",
    "    # in the vectors together\n",
    "    \n",
    "    for i in range(len(v1)):\n",
    "        distance = distance + np.abs(v1[i]-v2[i])\n",
    "        \n",
    "    mean_distance = distance/len(v1)\n",
    "    \n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = [1, 2, 0.5]\n",
    "v2 = [0.9, 2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when computing with vectors one should **beware of foor loops!**. It is in general a bad idea to force the computer to do one thing at a time when it's very capable of doing computations in parallel. \n",
    "\n",
    "Here's a _much_ faster version of the above function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between the two \n",
    "    vectors v1 and v2 (that are of equal length)\n",
    "    \"\"\"\n",
    "    v1, v2 = np.array(v1), np.array(v2)\n",
    "    \n",
    "    distance = np.sum(np.abs(v1-v2))\n",
    "    mean_distance = distance/len(v1)\n",
    "    \n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to measure the performance of the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the mean absolute error achieved by our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error and root mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two similar measures that are widely used are **mean squared error**, which is the average of the _squared_ distances, and the **root mean squared error**, which is the square root of mean squared error. \n",
    "\n",
    "By squaring the distances between the points, instances where this distance is large become much more impactful on the performance measures (as the square of a large number is even larger). In other words, _outliers_ have more impact on one's performance measures, which is a good thing if outliers are important in the setting you find yourself in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they are as formulas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE = \\frac 1n \\sum_i^n |\\hat{y_i} - y_i|^2$$\n",
    "$$RMSE = \\sqrt{\\frac 1n \\sum_i^n |\\hat{y_i} - y_i|^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between the two \n",
    "    vectors v1 and v2 (that are of equal length)\n",
    "    \"\"\"\n",
    "    v1, v2 = np.array(v1), np.array(v2)\n",
    "    \n",
    "    squared_distance = np.sum(np.abs(v1-v2)**2)\n",
    "    mean_squared_distance = squared_distance/len(v1)\n",
    "    \n",
    "    return mean_squared_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between the two \n",
    "    vectors v1 and v2 (that are of equal length)\n",
    "    \"\"\"\n",
    "    v1, v2 = np.array(v1), np.array(v2)\n",
    "    \n",
    "    squared_distance = np.sum(np.abs(v1-v2)**2)\n",
    "    root_mean_squared_distance = np.sqrt(squared_distance/len(v1))\n",
    "    \n",
    "    return root_mean_squared_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `R^2`: The coefficient of determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _coefficient of determination_ is another way to measure the distance between the predicted labels and the true labels in our test set. It is computed from two quantities: the *residual sum of squares* **$SS_{res}$** and the *total sum of squares* **$SS_{tot}$**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "SS_{res} &= \\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    "SS_{tot} &= \\sum_i (y_i - \\bar{y})^2,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\bar{y}$ is the mean of the label values:\n",
    "$$\n",
    "\\bar{y} = \\frac1n \\sum_i y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for $R^2$ is then:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the predictions agree perfectly with the labels then $R^2 = 1$. If a model always predicts $\\bar{y}$, then it has $R^2 = 0$. A model that performs worse than this will have a negative $R^2$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y, yhat):\n",
    "    ymean = np.mean(y)\n",
    "    ssres = np.sum((y - yhat)**2)\n",
    "    sstot = np.sum((y - ymean)**2)\n",
    "    return 1 - ssres/sstot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics are extremely standard and can therefore of course also be found in scikit-learn. It's not necessary to make your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our RandomForestRegressor to some other models. Have a look at the scikit-learn documentation for more information (and inspiration): https://scikit-learn.org/stable/supervised_learning.html#supervised-learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(x=y_test, y=lr.predict(X_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mean_absolute_error(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mean_squared_error(y_test, lr.predict(X_test), squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "mean_squared_error(y_test, lr.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "r2_score(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ElasticNet()\n",
    "en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(x=y_test, y=en.predict(X_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mean_absolute_error(y_test, en.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mean_squared_error(y_test, en.predict(X_test), squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "mean_squared_error(y_test, en.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "r2_score(y_test, en.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=500, max_depth=4, learning_rate=0.2)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.regplot(x=y_test, y=gb.predict(X_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mean_absolute_error(y_test, gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mean_squared_error(y_test, gb.predict(X_test), squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "mean_squared_error(y_test, gb.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "r2_score(y_test, gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen some basic ideas for constructing and evaluating regression models. In Assignment #1, you'll go into greater detail while building your own models to predict housing prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
