{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 12.08.2022, A. S. Lundervold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-versus-regression\" data-toc-modified-id=\"Classification-versus-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Classification versus regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-or-regression?\" data-toc-modified-id=\"Classification-or-regression?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Classification or regression?</a></span></li></ul></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Get-the-data:-the-diabetes-data-set\" data-toc-modified-id=\"Get-the-data:-the-diabetes-data-set-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Get the data: the diabetes data set</a></span></li><li><span><a href=\"#Explore-the-data\" data-toc-modified-id=\"Explore-the-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Explore the data</a></span></li><li><span><a href=\"#Create-training-and-test-sets\" data-toc-modified-id=\"Create-training-and-test-sets-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create training and test sets</a></span></li><li><span><a href=\"#Training-a-classifier\" data-toc-modified-id=\"Training-a-classifier-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training a classifier</a></span></li><li><span><a href=\"#Evaluating-models-/-performance-measures\" data-toc-modified-id=\"Evaluating-models-/-performance-measures-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluating models / performance measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-and-different-sorts-of-errors\" data-toc-modified-id=\"Accuracy-and-different-sorts-of-errors-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Accuracy and different sorts of errors</a></span></li><li><span><a href=\"#Validation-set-and-model-selection\" data-toc-modified-id=\"Validation-set-and-model-selection-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Validation set and model selection</a></span></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Cross-validation</a></span></li><li><span><a href=\"#Confusion-matrix\" data-toc-modified-id=\"Confusion-matrix-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Confusion matrix</a></span></li><li><span><a href=\"#Precision,-recall-and-specificity\" data-toc-modified-id=\"Precision,-recall-and-specificity-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Precision, recall and specificity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Connection-to-the-confusion-matrix\" data-toc-modified-id=\"Connection-to-the-confusion-matrix-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>Connection to the confusion matrix</a></span></li><li><span><a href=\"#In-scikit-learn\" data-toc-modified-id=\"In-scikit-learn-7.5.2\"><span class=\"toc-item-num\">7.5.2&nbsp;&nbsp;</span>In scikit-learn</a></span></li></ul></li><li><span><a href=\"#$F_1$-score\" data-toc-modified-id=\"$F_1$-score-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>$F_1$ score</a></span></li><li><span><a href=\"#Precision/recall-tradeoff\" data-toc-modified-id=\"Precision/recall-tradeoff-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Precision/recall tradeoff</a></span></li><li><span><a href=\"#Comparing-classifiers\" data-toc-modified-id=\"Comparing-classifiers-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Comparing classifiers</a></span></li></ul></li><li><span><a href=\"#Final-test-of-classifiers\" data-toc-modified-id=\"Final-test-of-classifiers-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Final test of classifiers</a></span></li><li><span><a href=\"#Summary-so-far\" data-toc-modified-id=\"Summary-so-far-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Summary so far</a></span></li><li><span><a href=\"#Next\" data-toc-modified-id=\"Next-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Next</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook and the next goes through some core concepts related to **classification** in machine learning. In a later notebook we'll do the same for **regression**. \n",
    "\n",
    "It's based on the textbook's Chapter 3. You should also have a look at Geron's notebook: https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll base our discussion on two data sets: the diabetes data set from a previous notebook and the famous benchmark data set **MNIST**.\n",
    "\n",
    "The diabetes data set will represent the case of **binary classification problem**, i.e., where there are only two possible output predictions (True or False), while the MNIST data set represents **multiclass classification problems**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be covered in two notebooks: \n",
    "\n",
    "* `DAT158-1.4-Binary_classification` (the current notebook)\n",
    "* `DAT158-1.5-Multiclass_classification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before we start, what is classification and regression, and how do you distinguish them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification versus regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**\n",
    "\n",
    "In classification we want to decide which of N classes an input belongs to (think of the dogs from the slides in the first lecture). We'll do this by training models on a set of examples of each class. \n",
    "\n",
    "Essentially, it's about finding a *good* function from the inputs $X$Â to a set of labels: \n",
    "\n",
    "$$f: X \\longrightarrow \\{1, \\dots, N\\}.$$\n",
    "\n",
    "Note that classification is a *discrete* problem: each input belongs to a class, and the set of classes cover all possibilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's one of the two main forms of **supervised learning**, the other one being **regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**\n",
    "\n",
    "Regression is about finding a function that maps from the inputs to a *continuous* set of numbers:\n",
    "\n",
    "$$f: X \\longrightarrow \\mathbb{R}.$$\n",
    "\n",
    "Think for example of a model tasked with predicting housing prices from a set of descriptive features (e.g. square footage, number of bedrooms, location, etc). The goal then is to construct a function mapping from a set of features of a house to its price. \n",
    "\n",
    "More generally, you have a set of $x$-values (inputs) and corresponding outputs as real numbers ($y$-values). When given a new, previously unseen $x$-value, what $y$-value should you predict? What is the best interpolating function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification or regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decide whether you have a classification or a regression problem? Ask yourself\n",
    "> Is there continuity on the possible predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting a price of 1.500.000 versus 1.500.001 makes a very small difference, suggesting that you're faced with a regression problem. Predicting that an image contains object number 12 versus object number 13 likely makes a huge difference, and you should probably treat this as a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import our standard framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory in which to store data\n",
    "NB_DIR = Path.cwd()       # Set NB_DIR to be the current working directory\n",
    "DATA = NB_DIR/'data'      # The data dir is the subdirectory 'data' under NB_DIR\n",
    "\n",
    "DATA.mkdir(exist_ok=True) # Create the data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data: the diabetes data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/alu042/DAT158-2022/blob/main/notebooks/assets/diabetes.jpg' width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll dowload the data as we did in the previous notebook. You're encouraged to go back to that notebook to refresh your memory of the data set and its characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://assets.datacamp.com/production/course_1939/datasets/diabetes.csv'\n",
    "urllib.request.urlretrieve(url, DATA/'diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pandas to inspect and process text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we recall, there are 768 instances in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some are labeled as diabetes some as not-diabetes, with more not-diabetes than diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes['diabetes'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a description of each feature (from <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">Kaggle</a>):\n",
    "- Pregnancies: number of times pregnant\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- Diastolic: Diastolic blood pressure (mm Hg)\n",
    "- Triceps: Triceps skin fold thickness (mm)\n",
    "- Insulin: 2-hour serum insulin (mu U/ml)\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "- DPF: Diabetes pedigree function\n",
    "- Age: Age (years)\n",
    "- Diabetes: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store the features in 'X' and the labels in 'y'. Our goal is to approximate the function mapping X to y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT158-2022/blob/main/notebooks/assets/f_xy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes.drop('diabetes', axis=1)\n",
    "y = diabetes['diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the data, the first step is to set aside a test set. The data in the test set will not be used for anything else than the final test of our machine learning model's accuracy. Using the test set during model design is a huge no-no since this will give a biased performance estimate when evaluating our model on the test set (i.e. we'll overfit to the test data set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're not interested in how well our models perform on the training set, what we're really after is how well they generalize to unseen data. \n",
    "\n",
    "The test set is meant to simulate unseen data (and should therefore not be touched when constructing and tuning our models). \n",
    "\n",
    "<img width=50% src=\"https://github.com/alu042/DAT158-2022/blob/main/notebooks/assets/testsplit.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly split off 25% of the data to be used as a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The random state is set to ensure that we get the same random split every time the cell is executed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 576 instances for training, 192 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of choices when building our model, something we'll learn more about later the course. For now, let's use `SGDClassifier` as a \"black box\", without studying how it works behind the scenes (but feel free to read more about the model [here](http://scikit-learn.org/stable/modules/sgd.html)). Our focus now is on classification in general, not on specific models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model:\n",
    "sgd_clf = SGDClassifier(random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained on the training data, and we can use it to make predictions for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 192 predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 192 correct answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put them next to each other and print out the first few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(y_test, y_pred))[:10] # \"Zip\" the two above arrays and display the first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model is correct some times, incorrect others. \n",
    "\n",
    "> **But how good is the model, really?** \n",
    "\n",
    "We need ways to evaluate and validate models.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models / performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, when evaluating the performance of a model one should really ask \"*What is the end goal for my system\"?* We're supposed to create systems that are useful in some context, as part of a larger system, which typically has a higher-level goal that our system should aim to optimize. Perhaps it's worth sacrificing accuracy for speed, or not getting a lot of useless clicks that don't lead to sales?\n",
    "\n",
    "However, we won't think about these broader context matters in these toy problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and different sorts of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification a common metric is accuracy: what fraction of our predictions were correct? \n",
    "\n",
    "But there are different kinds of errors in binary classification: if we classify something as belonging to the positive class we can either be correct (**true positive**) or incorrect (**false positive**). If we classify something as negative we can either be correct (**true negative**) or incorrect (**false negative**). \n",
    "\n",
    "What types of errors we care most about depends on the task: if we're for example diagnosing a treatable condition in patients we should do everything we can to reduce the rate of false negatives. While perhaps still keeping an eye on the false positive rate because a positive diagnosis could lead to invasive and extensive further testing for the patient.\n",
    "\n",
    "In spam filtering we care most about not marking important emails as spam, i.e. we want a low false positive rate (non-spam marked as spam) even if it means a higher false negative rate (some spam emails ending up in our inbox). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing accuracy, we can use the `accuracy_score` function from `sklearn.metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model makes the correct prediction approximately 54% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to play around with different models and their \"settings\" (i.e. changeable parameters) to decide which model to use. This is called **model selection**.\n",
    "\n",
    "\n",
    "During model selection it is very important to *not* base ones decisions on test set performance! Otherwise we'll have no way to produce an unbiased estimate of how well the final model will generalize to new data, as we've used information from our test set to design the model.\n",
    "\n",
    "We can deal with this issue by splitting the training set into two: a data set used for training (still called the training set) and a data set for evaluating performance while trying out various possible models and settings, called a **validation set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen this idea used earlier in the course, and we know how to do this using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT158-2022/blob/main/notebooks/assets/trainvaltestset.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!**\n",
    "- Split `(X_train, y_train)` into two parts: one for training, one for validation. You can call the new data sets and label sets `X_train_new, X_val, y_train_new, y_val`. Hint: Use the `train_test_split` function.\n",
    "- Train your model again on the new training set, and compute its accuracy on the validation set using the `score` method (in the case of classifiers, this computes the `accuracy_score` automatically, as we did above): `model.score(X_val, y_val)` \n",
    "- Is the result good? What accuracy would a random guesser have?\n",
    "- Play around with some of the parameters of `SGDClassifier` and try to improve the result on the validation set. Once you've found some parameters that seem good, test on the test set: `model.predict(X_test)` and `model.score(X_test, y_test)` (If you want to be thorough, use grid search to test parameter combinations). Note that as mentioned above, it is very important to not use the test data set until you've done all of the model selection work (selecting which model to use, what parameter settings it should have, and so on). Otherwise the models will be tuned on the test data set, and it will no longer give a realistic simulation of new data coming in, and therefore the score achieved on the test set will provide a biased generalization performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another frequently used way to do validation is *cross-validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to randomly split the training set into several parts, so called *folds*. Say into K folds, for example. Then train a model K times, each time using a different fold for evaluation and training on the remaining K-1. The average score for the K runs is used to estimate the model's performance. \n",
    "\n",
    "This means that *each sample in the training set is part of the training set K-1 times and the evaluation set once*. \n",
    "\n",
    "<img src=\"https://github.com/alu042/DAT158-2022/blob/main/notebooks/assets/K-fold_cross_validation_EN.jpg\"><br>\n",
    "<span style=\"font-size:70%\">Image from <a href=\"https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.jpg\">Wikipedia</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important advantage of this approach over the one above is that it doesn't waste as much training data. Unless you have plenty of data, cross-validation is the preferred method for estimating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation also provides a more thorough test than splitting the data into a training set and a test set. The `train_test_split` procedure sets aside a fixed random subset of the data as a test set. If we're unlucky, all the difficult examples end up in the training set, while the test set contains only easy ones. That would lead to an overestimate of the true performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the special case of cross-validation where K is set to the number of data points in the training set is called *leave-one-out*. Each fold is then a single sample.\n",
    "\n",
    "The result of this K-fold cross validation procedure is an array of K evaluation scores.\n",
    "\n",
    "> We'll use the cross-validation approach in the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(sgd_clf, X_train, y_train, cv=5, \n",
    "                            scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy of our model on the five folds is about 57.3%, which on the surface looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but is it really? Remember that there were more non-diabetes instances than diabetes instances. \n",
    "\n",
    "In fact, there are in total 500 non-diabetes instances and 268 diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes['diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if we simply guessed that everyone belongs to the class non-diabetes, we would achieve an accuracy of..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy = [0,]*len(y_test) # This creates a list of 0 with the same length as `y_test`\n",
    "accuracy_score(y_test, y_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...64%! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're dealing with an **unbalanced dataset** (also called a *skewed dataset*). In such cases accuracy is typically not a very informative measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that's always predicting non-diabetes will have an accuracy of approximately 64%. That makes our model less impressive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there's a better way to evaluate the model than merely computing the accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn:** Many machine learning models, like the `SGDClassifier`, perform better if the input data is properly scaled. Try using the `StandardScaler` in scikit-learn to standardize the training data. Then feed it through `cross_val_score`. You could also try the `MinMaxScaler`. \n",
    "\n",
    "> Hint: `from sklearn.preprocessing import StandardScaler, MinMaxScaler`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table summarizing the results from a classification model's predictions. In what way is the classifier \"confused\"? What errors does it tend to make?\n",
    "\n",
    "The idea is to simply count the number of times instances of a certain class is classified as the various classes in your problem. In our case, how often the diabetes and non-diabetes are correctly classified, how often a non-diabetes is misclassified as diabetes and how often a diabetes is misclassified as a diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some produce some predictions to compare with the true classes. We can use `cross_val_predict`, which works similarly to `cross_val_score` except that it returns predictions based on all the K folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the matrix represents a class: non-diabetes and diabetes. Each column represents a predicted class. \n",
    "\n",
    "The first row tells us that 226 instances were correctly classified as non-diabetes (**true negatives**). There were 151 instances classified as diabetes that where really non-diabetes (**false positives**).\n",
    "\n",
    "The second row tells us that 95 diabetes instances were wrongly labelled as non-diabetes (**false negatives**), while the remaining 104 diabetes instances were correctly classified (**true positives**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** How would the confusion matrix of a perfect classifier be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers tell us a lot about classifier performance. But sometimes it's useful to quantify (i.e. compute numbers) how much our classifier deviates from a perfect classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall and specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification problems, i.e. where there are only two possible classes, we can use **precision**, **recall** and **specificity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to first introduce some notation:\n",
    "\n",
    "- P = All positive data points\n",
    "- N = All negative data points\n",
    "- TP = True positives\n",
    "- FP = False positives\n",
    "- TN = True negatives\n",
    "- FN = False negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** What are the values of these numbers for the classifier above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** of a binary classifier is **the proportion of the positive predictions that were actually correct**. In other words, \n",
    "\n",
    "$$precision = \\frac{\\mbox{true positives}}{\\mbox{positive predictions}} = \\frac{TP}{TP + FP}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** What's the precision of the classifier above? What's the precision of a _perfect_ classifier? If a classifier has perfect precision, is it necessarily a good classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **recall** of a binary classifier is **the proportion of actual positives that were correctly identified**. In other words, \n",
    "\n",
    "$$recall = \\frac{\\mbox{true positives}}{\\mbox{all actual positives}} = \\frac{TP}{P} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "Recall is sometimes also called the **true positive rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** What's the recall of the classifier above? What's the recall of a _perfect_ classifier? If a classifier has perfect recall, is it necessarily a good classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **specificity** of a binary classifier is **the proportion of actual negatives that were correctly identified**. In other words,\n",
    "\n",
    "$$specificity = \\frac{\\mbox{true negatives}}{\\mbox{all actual negatives}} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "Specificity is sometimes also called the **true negative rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** What's the specificity of the classifier above? What's the specificity of a _perfect_ classifier? If a classifier has perfect specificity, is it necessarily a good classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=40% src=\"https://github.com/alu042/DAT158-2022/blob/main/notebooks/assets/Precisionrecall.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=font-size:80%>Illustration from Wikipedia</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the connection between TP, FP, FN and TN and the confusion matrix to compute the precision, recall and specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a way to pick out elements of a two-dimensional array\n",
    "cm[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..or slightly more conveniently:\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision}\\n Recall: {recall}\\n Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, scikit-learn has this functionality built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** What is more important, high precision or high recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** A question to ponder: How would you define precision and recall if you have more than two classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $F_1$ score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you typically care about both precision and recall, it can sometimes make sense to combine them into one number (for example when comparing two classifiers). The **$F_1$ score** is one way, defined as a sort of average of precision and recall. Not the usual average, but what's called the *harmonic mean*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why you would use the harmonic mean instead of the standard (arithmetic) mean. The main reason is that we want to output a low score for a classifier that has either very bad precision or very bad recall (i.e. one of them close to 0). Such classifiers should be punished by assigning them a very low score. \n",
    "\n",
    "For example, if the precision is 0.0001 and the recall is 1, the $F_1$ score will be close to 0, while the artihmetic mean would be (0.0001 + 1) / 2 ~ 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Say you have two classifiers A and B, with\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mbox{precision}_A = 0.7,\\quad &\\mbox{recall}_A = 0.6 \\\\\n",
    "\\mbox{precision}_B = 0.3,\\quad &\\mbox{recall}_B = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> What are the corresponding arithmetic means of the two models? What are their $F_1$ scores? You'll notice that model B's bad precision results in a lower $F_1$ compared to model A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $F_1$ score of 1.0 means perfect precision *and* perfect recall (i.e. a perfect classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Remember that I asked you to try to construct a better `SGDClassifier` model by normalizing the data before it is put into the model. If you haven't done so already, please try. Then run your predictions through the above set of evaluation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a solution (uncomment and run the below cell). Don't peek until you've tried yourself. :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if colab:\n",
    "#    !wget https://raw.githubusercontent.com/alu042/DAT158-2022/master/notebooks/solutions/1.4-sgd_normalized.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/1.4-sgd_normalized.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If you're using Colab, copy the output of the above cell into a new code cell and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If colab: paste the above output here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the classifier trained on the normalized data in what follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = sgd_clf_std\n",
    "\n",
    "X_train = X_train_std\n",
    "X_test = X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/recall tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, **it's typically impossible to achieve both high precision and high recall simultaneously**. They are typically competing quantities: when one is high the other is low. Working to improve the precision often lowers the model's recall, and vice-versa. You have to trade one for the other.\n",
    "\n",
    "Let's try to understand why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier with perfect recall is easy to construct: just predict that everything is diabetes! Then there are no false negatives and the recall is 1.0. Perfect. Except, the precision will be rubbish.\n",
    "\n",
    "To get high precision, make a classifier predict *one* instance as diabetes that you're very sure is correct, and the rest as non-diabetes. Then the number of false positives will likely be 0, and the precision 1.0. The recall will however be really bad as there will be many false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this principle in action for our `SGDClassifier`. But we'll have to discuss what's going on behind the scenes when we ran `sgd_cl.fit` on our training data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fitting_ or _training_ the `SGDClassifier` means finding a _hyperplane_, or a _boundary_, in the data space that separates the diabetes instances from the non-diabetes as much as possible. \n",
    "\n",
    "When feeding `SGDClassifer` a data point it decides whether it is a diabetes or a non-diabetes based on the _distance_ from the data point to this hyperplane. By default, if the data point has a negative distance to the plane, the model predicts that it's from the negative class (a non-diabetes in our case). If the distance is positive, it's predicted to be from the positive class (a diabetes instances). In other words, the **decision threshold** is set to 0 by default. \n",
    "\n",
    "Furthermore, the distance from the data point to the hyperplane can be interpreted as how _certain_ the model is about its prediction. The larger the distance, the more certain the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Where you set the threshold makes a big difference***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some experiments on an example to investigate further. We'll use the first instance in the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label\n",
    "y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SGDClassifier` has a method that computes the distance of a data point from the hyperplane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_example_score = sgd_clf.decision_function([X_test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_example_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_example_pred = (y_example_score > threshold)\n",
    "print(f\"Is it a diabetes instance? {y_example_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -10\n",
    "y_example_pred = (y_example_score > threshold)\n",
    "print(f\"Is it a diabetes instance? {y_example_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that increasing the threshold for classifying something as diabetes makes the model less likely to do so. *This increases the precision of the classifier, but reduces its recall.* Decreasing the threshold has the opposite effect: higher recall, lower precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this.\n",
    "\n",
    "First we compute the decision function score for all the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train, cv=2, method='decision_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 576 scores, one for each training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute and plot the precisions and recalls for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using such plots it's possible to select the threshold value that gives the best precision/recall tradeoff for your task.\n",
    "\n",
    "Are false negatives very bad (like in medical diagnosis)? Select a low threshold to get high recall and OK precision. Are false positives especially costly (like in spam detection)? Go for a threshold that gives you high precison and OK recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's very easy to create a classifier with any precision you want, as long as you don't care about recall. You just have to select a suitable threshold. \n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_precise = (y_scores > 18)\n",
    "precision_score(y_train, y_train_pred_precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_train_pred_precise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If someone says \"let's reach 90% precision!\", you should ask, \"at what recall?\" &mdash; Aurelien Geron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the F1 score is a kind of mean between precision and recall, it can be used to catch models who trades off a lot of precision for high recall or vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, (y_scores > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, (y_scores > 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, (y_scores < -10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics we've discussed above gives us ways to compare classifiers. We can for example use the F1 score. Computing such numbers are often more convenient than looking at curves, and also enables automatic model selection.\n",
    "\n",
    "We can try this out by creating another model to solve our classification task: a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Get predictions\n",
    "y_train_preds_rf = cross_val_predict(forest_clf, X_train, y_train, cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_rf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 score for RandomForestClassifer: {f1_score(y_train, y_train_preds_rf)}\\n\")\n",
    "print(f\"F1 score for SGDClassifier: {f1_score(y_train, y_train_std_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to prefer the Random Forest classifier (unless of course other considerations besides what's captured by the F1 score pulls in the direction of the SGDClassifier...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_preds_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_std_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that the confusion matrix for the random forest is in general better than for the SGD classifier. But note also that the SGDClassifier actually manages to detect one more true positives than the random forest classifier.\n",
    "\n",
    "Note also that the random forest is much slower in producing its predicitons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "cross_val_score(forest_clf, X_train, y_train, cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such findings may lead one to actually prefer a model that is weaker in some respects, depending on practical considerations about the \"bigger picture\". What the model is supposed to be used for, and as part of what pipeline or workflow.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these experiments, we can make a final performance check against the test data. This will provide our estimate of the model's generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn!** Plot the confusion matrix for the test data and the Random Forest predictions. Compute its precision, recall, specificity, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the things we've learned so far:\n",
    "\n",
    "- How to train a binary classifier\n",
    "- Choosing appropriate metrics for the task\n",
    "- Evaluating classifiers using cross-validation\n",
    "- Selecting appropriate precision/recall tradeoffs\n",
    "- Comparing models using F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we'll take a look at multiclass classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
